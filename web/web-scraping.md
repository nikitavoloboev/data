# Web scraping

## Links

- [What is a present-day web scraping in 2020?](https://dataflowkit.com/blog/what-is-a-present-day-web-scraper/)
- [Dataflow Kit](https://dataflowkit.com/) - Web scraping. Data extraction tools
- [Easy web scraping with Scrapy (2019)](https://www.scrapingbee.com/blog/web-scraping-with-scrapy/)
- [A guide to Web Scraping without getting blocked in 2020](https://www.scrapingbee.com/blog/web-scraping-without-getting-blocked/)
- [Crawlab](https://github.com/crawlab-team/crawlab) - Distributed web crawler admin platform for spiders management regardless of languages and frameworks.
- [hakrawler](https://github.com/hakluke/hakrawler) - Simple, fast web crawler designed for easy, quick discovery of endpoints and assets within a web application.
- [JobFunnel](https://github.com/PaulMcInnis/JobFunnel) - Tool for scraping job websites, and filtering and reviewing the job listings.
- [You-Get](https://github.com/soimort/you-get) - Tiny command-line utility to download media contents (videos, audios, images) from the Web.
- [Universal Reddit Scraper](https://github.com/JosephLai241/Universal-Reddit-Scraper) - Scrape Subreddits, Redditors, and comments on posts. A command-line tool written in Python.
- [Gerapy](https://github.com/Gerapy/Gerapy) - Distributed Crawler Management Framework Based on Scrapy, Scrapyd, Django and Vue.js.
- [Ask HN: Best practices for ethical web scraping? (2020)](https://news.ycombinator.com/item?id=22778089)
- [Newscatcher](https://newscatcherapi.com/) - Programmatically collect normalized news from (almost) any website. ([Code](https://github.com/kotartemiy/newscatcher))
- [scrapio](https://github.com/Koshqua/scrapio) - Simple and easy-to-use scraper and crawler in Go.
- [Colly](https://github.com/gocolly/colly) - Elegant Scraper and Crawler Framework for Golang.
- [Python Web Scraping with Virtual Private Networks (2020)](https://tech.marksblogg.com/python-scraper-wireguard-vpn-ssh-proxy.html)
- [extract-news-api](https://github.com/kotartemiy/extract-news-api) - Flask code to deploy an API that pulls structured data from online news articles.
- [Web Scraper](https://web.scraper.workers.dev/) - Scrape websites for text by CSS selector.
- [List all the broken links on your website](https://gist.github.com/mdamien/7b71ef06f49de1189fb75f8fed91ae82)
- [Creating a Robust, Reusable Link-Checker (2020)](http://adventures.michaelfbryan.com/posts/linkchecker/)
- [micawber](https://github.com/coleifer/micawber) - Small library for extracting rich content from urls.
- [Spider Pro](https://tryspider.com/) - Easy and cheap way to scrape the internet. ([HN](https://news.ycombinator.com/item?id=21215484))
- [Website Sitemap Parser](https://github.com/berkmancenter/mediacloud-ultimate-sitemap-parser)
- [rget](https://github.com/merklecounty/rget) - Download URLs and verify the contents against a publicly recorded cryptographic log.
- [yarl](https://github.com/aio-libs/yarl) - Yet another URL library.
- [Apify](https://apify.com/) - Web Scraping, Data Extraction and Automation.
- [Gumbo](https://github.com/google/gumbo-parser) - Pure-C HTML5 parser.
