---
title: JAX
---

# [JAX](https://github.com/google/jax)

[Differential Programming with JAX course](https://ericmjl.github.io/dl-workshop/) is nice. [Meta Optimal Transport](https://github.com/facebookresearch/meta-ot) is nice JAX repo to run/study.

## Links

- [audax](https://github.com/SarthakYadav/audax) - Home for audio ML in JAX. Has common features, learnable frontends, pretrained supervised and self-supervised models.
- [tinygp](https://github.com/dfm/tinygp) - Extremely lightweight library for building Gaussian Process models in Python, built on top of jax.
- [GPJax](https://github.com/thomaspinder/GPJax) - Didactic Gaussian process package for researchers in Jax.
- [Mctx](https://github.com/deepmind/mctx) - Monte Carlo tree search in JAX.
- [Pipelined Swarm Training](https://github.com/kingoflolz/swarm-jax) - Swarm training framework using Haiku + JAX + Ray for layer parallel transformer language models on unreliable, heterogeneous nodes.
- [JAX MuZero](https://github.com/Hwhitetooth/jax_muzero) - JAX implementation of the MuZero agent.
- [Jax Influence](https://github.com/google-research/jax-influence) - Scalable implementation of Influence Functions in JaX.
- [BlackJAX](https://github.com/blackjax-devs/blackjax) - Library of samplers for JAX that works on CPU as well as GPU. ([Twitter](https://twitter.com/blackjax_mcmc))
- [GPax](https://github.com/google-research/gpax) - Jax/Flax codebase for Gaussian processes including meta and multi-task Gaussian processes.
- [jax-fenics-adjoint](https://github.com/IvanYashchuk/jax-fenics-adjoint) - Differentiable interface to FEniCS/Firedrake for JAX using dolfin-adjoint/pyadjoint.
- [jax-ekf](https://github.com/brentyi/jax-ekf) - Generic EKF, with support for non-Euclidean manifolds.
- [PaLM - Jax](https://github.com/lucidrains/PaLM-jax) - Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways - in Jax.
- [Pre-trained image classification models for Jax/Haiku](https://github.com/abarcel/haikumodels)
- [Flaxformer: transformer architectures in JAX/Flax](https://github.com/google/flaxformer)
- [KFAC-JAX - Second Order Optimization with Approximate Curvature in JAX](https://github.com/deepmind/kfac-jax)
- [flowjax](https://github.com/danielward27/flowjax) - Normalizing flow implementations in jax.
- [Jax3D](https://github.com/google-research/jax3d) - Library for neural rendering in Jax and aims to be a nimble NeRF ecosystem.
- [DALL·E 2 in JAX](https://github.com/lucidrains/DALLE2-jax)
- [JAXNS](https://github.com/Joshuaalbert/jaxns) - Nested sampling in JAX.
- [AUX](https://github.com/deepmind/dm_aux) - Audio processing library in JAX, for JAX.
- [Nice DeepMind Jax libraries](https://twitter.com/DeepMind/status/1517146462571794433)
- [Machine Learning with JAX - From Zero to Hero (2021)](https://www.youtube.com/playlist?list=PLBoQnSflObckOARbMK9Lt98Id0AKcZurq)
- [Flax](https://github.com/google/flax) - Neural network library for JAX designed for flexibility. ([Docs](https://flax.readthedocs.io/en/latest/))
- [JAX talks by HuggingFace](https://www.youtube.com/playlist?list=PLo2EIpI_JMQtQrEduYXbRz4X50mTiOi8S)
- [Homomorphic Encryption in JAX](https://github.com/nkandpa2/he_jax)
- [JAX implementation of Learning to learn by gradient descent by gradient descent](https://github.com/teddykoker/learning-to-learn-jax)
- [Normalizing Flows in JAX](https://github.com/ChrisWaites/jax-flows)
- [Big Vision](https://github.com/google-research/big_vision) - Designed for training large-scale vision models on Cloud TPU VMs. Based on Jax/Flax libraries.
- [Jax vs. Julia (Vs PyTorch) (2022)](https://kidger.site/thoughts/jax-vs-julia/) ([HN](https://news.ycombinator.com/item?id=31263516))
- [minGPT in JAX](https://github.com/mgrankin/minGPT)
- [flaxvision](https://github.com/rolandgvc/flaxvision) - Selection of neural network models ported from torchvision for JAX & Flax.
- [JAX version of clip guided diffusion scripts](https://github.com/nshepperd/jax-guided-diffusion)
- [Functorch](https://github.com/pytorch/functorch) - Jax-like composable function transforms for PyTorch. ([HN](https://news.ycombinator.com/item?id=31424588))
- [Ninjax](https://github.com/danijar/ninjax) - Module system for JAX that offers full state access and allows to easily combine modules from other libraries.
- [Functional Transformer](https://github.com/awf/functional-transformer) - Pure-functional implementation of a machine learning transformer model in Python/JAX.
- [JAX + Units](https://github.com/dfm/jpu) - Provides and interface between JAX and Pint to allow JAX to support operations with units.
- [Infinite Recommendation Networks (∞-AE) in JAX](https://github.com/noveens/infinite_ae_cf)
- [Differential Programming with JAX course](https://ericmjl.github.io/dl-workshop/) ([Code](https://github.com/ericmjl/dl-workshop))
